{
  "name": "Disaster detection via twitter",
  "tagline": "Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)",
  "body": "###1. Project summary\r\n---\r\n**Goal:** Create a predictive model to classify a tweet as pertaining to a disaster event or not, *solely* based on the tweet text.\r\n\r\n**Data:** 10,876 classified tweet entries in the \"Disasters on Social Media\" data set from CrowdFlower at http://www.crowdflower.com/data-for-everyone. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.\r\n\r\n**Machine learning tools** \r\n\r\n1. **Natural Language Toolkit (NLTK)** - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.\r\n\r\n2. **GENSIM** - For seamless transformation from a high dimensional \"bag of words\" (BOW) feature space to a lower dimensional \"topic\" space (each topic expresses discovered relations amongst the words in the Tweet data set using Latent Semantic Indexing or LSI).\r\n\r\n3. **Scikit-Learn** - For trivial implementation the final Logistic Regression model which predicts \"disaster\" or \"not disaster\" based on each Tweets \"topic\" representation.\r\n\r\n###2. Clean and tokenize all the tweets\r\n---\r\n1. Convert hyphens and apostrophes from utf8 to ascii\r\n2. Remove all remaining utf8 characters\r\n3. Clean any HTML tags using HTMLParser module\r\n4. Break on hyphens\r\n5. Employ NLTK for initial tweet tokenization\r\n  * Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)\r\n    * In general, the english language has at most three identical letters in a row\r\n6. Introduce special tokens\r\n  * **|-num-|** for numbers (detect comma separation as in 1,000,000)\r\n  * **|-num_alpha-|** for mixed numerical and alphabetical (maybe useful for interstates, planes, trains, ...) \r\n    * Make sure not to convert handles! These are typically mixed numeric and alphabetic\r\n  * **|-num_units-|** for zero, one, ..., ten\r\n  * **|-num_tens-|** for ten, twenty, ..., ninety\r\n  * **|-num_scales-|** for hundred, thousand, ..., billion\r\n  * **|-website-|** for any hyperlinks\r\n8. Simplify common face emoticons down to just eyes and mouth (nose does not really convey emotion) and normalize (eyes first mouth second)\r\n  * Most common eyes : ; = 8\r\n  * Most common mouths ( ) [ ] d p { } / @ |\r\n9. Stem words using NLTK Porter stemmer (e.g., fires becomes fire)\r\n\r\n**NOTE:** stop words (i.e., high frequency words like \"and\") are not removed. Keeping them helps improve the final model performance--perhaps by allowing for better word-word relations to be encoded in the discovered \"topics\".\r\n\r\n\r\n###3. Mapping low frequency words onto high frequency analogs\r\n---\r\n**Low frequency words**: occur *once* in the whole cleaned and tokenized tweet data set.\r\n\r\n**High frequency words**: occur greater than *once* in the whole cleaned and tokenized tweet data set.\r\n\r\n**NOTE**: low frequency words also encompass those with no occurrence (as in a new tweet outside of the dataset). Word2vec would be used to map in that case as well.\r\n\r\nFor this I employ the pre-trained word2vec vectors from the Google News dataset. Word2vec is an unsupervised machine learning tool that creates vector representations of words by analyzing a large text corpus (here, Google News). Importantly, the similarity between two words can be quantified by the angle (theta in cartoon below) between the two word vectors.\r\n\r\nMapping a low frequency *stemmed* token via word2vec amounts to finding the most similar high frequency *stemmed* token by comparing all base *unstemmed* words. This procedure is depicted in the following cartoon where arrows represent a mapping and some actual examples (from mapping procedure carried out below) are shown.\r\n\r\n![mapping_visual2](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure.png)\r\n\r\n\r\n\r\n\r\n###6. Decide on a model dimensionality and check accuracy\r\n\r\n---\r\n\r\n**Model used**: logistic regression (see: https://en.wikipedia.org/wiki/Logistic_regression).\r\n\r\n**Why**: well suited to high dimensional problems.\r\n\r\n**Two tests across a range of dimensions**:\r\n\r\n1. k-fold cross validation error to assess how well the model will generalize to new tweets.\r\n  * Unfortunately this does not account well for testing generalizability to tweets with new (non-dictionary) tokens. Ideally, the word2vec mapping will remedy the relatively small dictionary.\r\n2. Training error to fit the whole dataset.\r\n\r\n![cross validation](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation.png)\r\n\r\n\r\n\r\n###8. Check out the \"topics\"\r\n---\r\n  1. Print out the top 10 topics with the top ten tokens they are composed of\r\n  2. Plot some topics against each other with colors to indicate class\r\n\r\n`Topic 0: 0.426*\"?\" + 0.234*\"#\" + 0.205*\"@\" + 0.196*\"|-no_w2v-|\" + 0.188*\"'\" + 0.185*\".\" + 0.183*\"the\" + 0.160*\"|-num-|\" + 0.158*\"i\" + 0.156*\"a\"`\r\n\r\n`Topic 1: -0.808*\"?\" + 0.160*\"|-num-|\" + 0.159*\"'\" + 0.148*\":\" + 0.127*\"#\" + 0.110*\"of\" + 0.107*\"in\" + 0.096*\"|-website-|\" + 0.093*\"famili\" + 0.093*\"...\"`\r\n\r\n###...\r\n\r\n`Topic 9: -0.242*\"kill\" + 0.231*\"#\" + 0.222*\"!\" + 0.212*\"|-num_alpha-|\" + -0.208*\"obama\" + -0.203*\"declar\" + -0.203*\"disast\" + -0.173*\"|-num_units-|\" + 0.167*\"bomb\" + -0.150*\"for\"`\r\n\r\n`Topic 10: -0.463*\"!\" + -0.321*\"obama\" + -0.312*\"declar\" + -0.287*\"disast\" + -0.208*\"for\" + -0.200*\"@\" + -0.185*\"saipan\" + -0.184*\"typhoon\" + 0.180*\".\" + -0.166*\"devast\"`\r\n\r\n![Topics](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics.png)\r\n\r\n\r\n###8. Proposed work",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}