{
  "name": "Disaster detection via twitter",
  "tagline": "Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)",
  "body": "###1. Project summary\r\n---\r\n**Goal:** Create a predictive model to classify a tweet as pertaining to a disaster event or not, *solely* based on the tweet text.\r\n\r\n**Data:** 10,876 classified tweet entries in the \"Disasters on Social Media\" data set from CrowdFlower at http://www.crowdflower.com/data-for-everyone. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.\r\n\r\n**Machine learning tools** \r\n\r\n1. **Natural Language Toolkit (NLTK)** - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.\r\n\r\n2. **GENSIM** - For seamless transformation from a high dimensional \"bag of words\" (BOW) feature space to a lower dimensional \"topic\" space via Latent Semantic Indexing or LSI.\r\n\r\n3. **Scikit-Learn** - Fitting of final Logistic Regression classification model to \"topics\" represented tweets.\r\n\r\n###2. Clean and tokenize all the tweets\r\n---\r\n1. Convert hyphens and apostrophes from utf8 to ascii\r\n2. Remove all remaining utf8 characters\r\n3. Clean any HTML tags using HTMLParser module\r\n4. Break on hyphens\r\n5. Employ NLTK for initial tweet tokenization\r\n  * Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)\r\n6. Introduce special tokens\r\n  * **|-num-|** for numbers (detect comma separation as in 1,000,000)\r\n  * **|-num_alpha-|** for mixed numerical and alphabetical (interstates, planes, trains) but not handles \r\n  * **|-num_units-|** for zero, one, ..., ten\r\n  * **|-num_tens-|** for ten, twenty, ..., ninety\r\n  * **|-num_scales-|** for hundred, thousand, ..., billion\r\n  * **|-website-|** for any hyperlinks\r\n8. Simplify common face emoticons down to just eyes and mouth (nose does not really convey emotion) and normalize (eyes first mouth second)\r\n  * Most common eyes : ; = 8\r\n  * Most common mouths ( ) [ ] d p { } / @ |\r\n9. Stem words using NLTK Porter stemmer (e.g., fires becomes fire)\r\n\r\n**NOTE:** stop words (like \"and\") are not removed as keeping them improves performance--perhaps by allowing for better word-word relations to be encoded in the discovered \"topics\".\r\n\r\n\r\n###3. Mapping low frequency words onto high frequency analogs\r\n---\r\n**Low frequency words**: occur *once* in the whole cleaned and tokenized tweet data set (also for unknown words).\r\n\r\n**High frequency words**: occur greater than *once* in the whole cleaned and tokenized tweet data set.\r\n\r\nEmploy pre-trained **word2vec** vectors from the Google News data set to quantify the similarity between two words. Low frequency *stemmed* tokens are replaced by most similar high frequency *stemmed* token (see cartoon).\r\n\r\n![mapping_visual2](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure.png)\r\n\r\n\r\n###4. Converting tweets into features for modeling\r\n---\r\n\r\n**Some useful definitions:**\r\n\r\n1. **Bag of words (BOW)**:  tweet vectors like [1,0,0,2,4,0,...,0] where each count specifies how many time a certain token appears in a tweet (sparse, i.e., mostly full of 0's). Please see https://en.wikipedia.org/wiki/Bag-of-words_model for more details.\r\n2. **Term frequencyâ€“inverse document frequency (TFIDF)**: rescaled BOW vectors where each word count is penalized (shrunk) in accord with how many of the documents contains the word (measure of uniqueness). Please see https://en.wikipedia.org/wiki/Tf%E2%80%93idf for more details.\r\n3. **Latent semantic indexing (LSI)**: constructs user specified number of \"Topics\" (linear combinations of the old TFIDF dimensions). These are constructed so so as to describe as much variance in the data as possible. Please see for https://en.wikipedia.org/wiki/Latent_semantic_analysis more details.\r\n\r\n**Dictionary**: uses all the tweets to get maximal word recognition. Tokens that appear only once are excluded.\r\n\r\n**Corpus**: only uses tweets of 100% confidence in classification to maximize reliability (some of the tweets with less than 100% appeared incorrectly labeled upon personal inspection).\r\n\r\n\r\n###5. Decide on a model dimensionality and check accuracy\r\n\r\n---\r\n\r\n**Model used**: logistic regression (see: https://en.wikipedia.org/wiki/Logistic_regression).\r\n\r\n**Why**: well suited to high dimensional problems.\r\n\r\n![cross validation](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation.png)\r\n\r\n**Conclusion**: Overall, D=250 seems good and is the dimensionality employed from here onward.\r\n\r\n\r\n###7. Make and test a model with desired dimensionality\r\n\r\n---\r\n\r\n**Test class separability**: create a k-fold receiver operating characteristic (ROC) curve (see: https://en.wikipedia.org/wiki/Receiver_operating_characteristic for more details).\r\n\r\n![ROC analysis](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/roc.png)\r\n\r\n**NOTE**: May want to tune threshold towards more false disaster predictions (better safe than sorry).\r\n\r\n\r\n\r\n###8. Check out the \"topics\"\r\n---\r\n**Top 10 topics with the top ten tokens**\r\n\r\n`Topic 0: 0.426*\"?\" + 0.234*\"#\" + 0.205*\"@\" + 0.196*\"|-no_w2v-|\" + 0.188*\"'\" + 0.185*\".\" + 0.183*\"the\" + 0.160*\"|-num-|\" + 0.158*\"i\" + 0.156*\"a\"`\r\n\r\n`Topic 1: -0.808*\"?\" + 0.160*\"|-num-|\" + 0.159*\"'\" + 0.148*\":\" + 0.127*\"#\" + 0.110*\"of\" + 0.107*\"in\" + 0.096*\"|-website-|\" + 0.093*\"famili\" + 0.093*\"...\"`\r\n\r\n###...\r\n\r\n`Topic 9: -0.242*\"kill\" + 0.231*\"#\" + 0.222*\"!\" + 0.212*\"|-num_alpha-|\" + -0.208*\"obama\" + -0.203*\"declar\" + -0.203*\"disast\" + -0.173*\"|-num_units-|\" + 0.167*\"bomb\" + -0.150*\"for\"`\r\n\r\n`Topic 10: -0.463*\"!\" + -0.321*\"obama\" + -0.312*\"declar\" + -0.287*\"disast\" + -0.208*\"for\" + -0.200*\"@\" + -0.185*\"saipan\" + -0.184*\"typhoon\" + 0.180*\".\" + -0.166*\"devast\"`\r\n\r\n**Topic 1 = non-disaster and  Topic 2 = disaster???**\r\n\r\n![Topics](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics.png)\r\n\r\n###8. Proposed work",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}