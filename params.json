{
  "name": "Disaster detection via twitter",
  "tagline": "Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)",
  "body": "###1. Project summary\r\n---\r\n**Goal:** Create a predictive model to classify a tweet as pertaining to a disaster event or not, *solely* based on the tweet text.\r\n\r\n**Data:** 10,876 classified tweet entries in the \"Disasters on Social Media\" data set from CrowdFlower at http://www.crowdflower.com/data-for-everyone. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.\r\n\r\n**Machine learning tools** \r\n\r\n1. **Natural Language Toolkit (NLTK)** - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.\r\n\r\n2. **GENSIM** - For seamless transformation from a high dimensional \"bag of words\" (BOW) feature space to a lower dimensional \"topic\" space via Latent Semantic Indexing or LSI.\r\n\r\n3. **Scikit-Learn** - Fitting of final Logistic Regression classification model to \"topics\" represented tweets.\r\n\r\n###2. Clean and tokenize all the tweets\r\n---\r\n1. Convert hyphens and apostrophes from utf8 to ascii\r\n2. Remove all remaining utf8 characters\r\n3. Clean any HTML tags using HTMLParser module\r\n4. Break on hyphens\r\n5. Employ NLTK for initial tweet tokenization\r\n  * Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)\r\n6. Introduce special tokens\r\n  * **|-num-|** for numbers (detect comma separation as in 1,000,000)\r\n  * **|-num_alpha-|** for mixed numerical and alphabetical (interstates, planes, trains) but not handles \r\n  * **|-num_units-|** for zero, one, ..., ten\r\n  * **|-num_tens-|** for ten, twenty, ..., ninety\r\n  * **|-num_scales-|** for hundred, thousand, ..., billion\r\n  * **|-website-|** for any hyperlinks\r\n8. Simplify common face emoticons down to just eyes and mouth (nose does not really convey emotion) and normalize (eyes first mouth second)\r\n  * Most common eyes : ; = 8\r\n  * Most common mouths ( ) [ ] d p { } / @ |\r\n9. Stem words using NLTK Porter stemmer (e.g., fires becomes fire)\r\n\r\n**NOTE:** stop words (like \"and\") are not removed as keeping them improves performance--perhaps by allowing for better word-word relations to be encoded in the discovered \"topics\".\r\n\r\n\r\n###3. Mapping low frequency words onto high frequency analogs\r\n---\r\n**Low frequency words**: occur *once* in the whole cleaned and tokenized tweet data set (also for unknown words).\r\n\r\n**High frequency words**: occur greater than *once* in the whole cleaned and tokenized tweet data set.\r\n\r\nEmploy pre-trained word2vec vectors from the Google News data set to quantify the similarity between two words. Low frequency *stemmed* tokens are replaced by most similar high frequency *stemmed* token (see cartoon).\r\n\r\n![mapping_visual2](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure.png)\r\n\r\n\r\n###4. Converting tweets into features for modeling\r\n---\r\n\r\n**Some useful definitions:**\r\n\r\n1. **Bag of words (BOW)**:  this is a vector representation of the form [1,0,0,2,4,0,...,0] where each dimension denotes a particular word and the number specifies how many times that word appears in a particular document (tweet in this case). There is a dimension for every word that appears at least twice (in this case) in the whole \"corpus\" of tweets (this is our \"dictionary\"). Since tweets are short the vectors are very sparse (mostly full of 0's). Please see https://en.wikipedia.org/wiki/Bag-of-words_model for more details.\r\n2. **Term frequencyâ€“inverse document frequency (TFIDF)**: rescaled BOW vectors where each word count is penalized (shrunk) in accord with how many of the documents contains the word--effectively adjusting the word counts for how unique each word is. For example, \"and\" will be heavily penalized, reflecting low information content and common usage. Since this is just a rescaling of each BOW element the new vectors are of the same *high* dimensionality. Please see https://en.wikipedia.org/wiki/Tf%E2%80%93idf for more details.\r\n3. **Latent semantic indexing (LSI)**: through a standard mathematical scheme from linear algebra--singular value decomposition (SVD--the TFIDF vectors are mapped onto a user specified, smaller dimensional space where the new dimensions are linear combinations of the old TFIDF dimensions. These new dimensions (commonly called \"topics\") are constructed so that they describe as much variance in the data as possible with rapidly diminishing returns for each new dimension. The outcome is a much more compact vectorial representation of each tweet. Please see for https://en.wikipedia.org/wiki/Latent_semantic_analysis more details.\r\n\r\n**Workflow**\r\n1. Create a dictionary and corpus\r\n  * Dictionary: use words from all of the tweets for more tokens\r\n  * Corpus: composed of the tweets with the greatest certainty in their classification. Convert to a BOW representation using the dictionary and then to a TFIDF analog.\r\n2. Create LSI features data frame of desired dimensionality from the dictionary and TFIDF tweet corpus (the dictionary just relates the new features to the actual words and is not really part of the dimensionality reduction)\r\n\r\n**Create data frames for constructing the dictionary and corpus**\r\n\r\n**Dictionary**: use all the tweets to get maximal word recognition, thus maximizing generalizability to new tweets. Tokens that appear only once are excluded.\r\n\r\n**Corpus**: only employ tweets of 100% confidence in classification to ensure the reliability of the predictive model. Some of the tweets with less than 100% appeared incorrectly labeled upon personal inspection.\r\n\r\n\r\n###5. Decide on a model dimensionality and check accuracy\r\n\r\n---\r\n\r\n**Model used**: logistic regression (see: https://en.wikipedia.org/wiki/Logistic_regression).\r\n\r\n**Why**: well suited to high dimensional problems.\r\n\r\n![cross validation](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation.png)\r\n\r\n**Conclusion**: Overall, D=250 seems good and is the dimensionality employed from here onward.\r\n\r\n\r\n###7. Make and test a model with desired dimensionality\r\n\r\n---\r\n\r\n1. Create a k-fold receiver operating characteristic (ROC) curve (see: https://en.wikipedia.org/wiki/Receiver_operating_characteristic for more details).\r\n  * This checks how well the model \"separates\" the two classes by plotting the \"true positive rate\" vs the \"false positive rate\" while moving the probability cutoff for classifying into one of the two categories (the default for classification is 50%).\r\n\r\n![ROC analysis](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/roc.png)\r\n\r\nThe ROC analysis suggests the model does a good job at separating out the classes. \r\n\r\n\r\n\r\n\r\n###8. Check out the \"topics\"\r\n---\r\n  1. Print out the top 10 topics with the top ten tokens they are composed of\r\n  2. Plot some topics against each other with colors to indicate class\r\n\r\n`Topic 0: 0.426*\"?\" + 0.234*\"#\" + 0.205*\"@\" + 0.196*\"|-no_w2v-|\" + 0.188*\"'\" + 0.185*\".\" + 0.183*\"the\" + 0.160*\"|-num-|\" + 0.158*\"i\" + 0.156*\"a\"`\r\n\r\n`Topic 1: -0.808*\"?\" + 0.160*\"|-num-|\" + 0.159*\"'\" + 0.148*\":\" + 0.127*\"#\" + 0.110*\"of\" + 0.107*\"in\" + 0.096*\"|-website-|\" + 0.093*\"famili\" + 0.093*\"...\"`\r\n\r\n###...\r\n\r\n`Topic 9: -0.242*\"kill\" + 0.231*\"#\" + 0.222*\"!\" + 0.212*\"|-num_alpha-|\" + -0.208*\"obama\" + -0.203*\"declar\" + -0.203*\"disast\" + -0.173*\"|-num_units-|\" + 0.167*\"bomb\" + -0.150*\"for\"`\r\n\r\n`Topic 10: -0.463*\"!\" + -0.321*\"obama\" + -0.312*\"declar\" + -0.287*\"disast\" + -0.208*\"for\" + -0.200*\"@\" + -0.185*\"saipan\" + -0.184*\"typhoon\" + 0.180*\".\" + -0.166*\"devast\"`\r\n\r\n![Topics](https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics.png)\r\n\r\n\r\n###8. Proposed work",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}