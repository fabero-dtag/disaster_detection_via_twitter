<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Disaster detection via twitter by rjadrich</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Disaster detection via twitter</h1>
        <p>Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)</p>

        <p class="view"><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View the Project on GitHub <small>rjadrich/disaster_detection_via_twitter</small></a></p>


        <ul>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h3>
<a id="1-project-summary" class="anchor" href="#1-project-summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Project summary</h3>

<hr>

<p><strong>Goal:</strong> Create a predictive model to classify a tweet as pertaining to a disaster event or not, <em>solely</em> based on the tweet text.</p>

<p><strong>Data:</strong> 10,876 classified tweet entries in the "Disasters on Social Media" data set from CrowdFlower at <a href="http://www.crowdflower.com/data-for-everyone">http://www.crowdflower.com/data-for-everyone</a>. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.</p>

<p><strong>Machine learning tools</strong> </p>

<ol>
<li><p><strong>Natural Language Toolkit (NLTK)</strong> - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.</p></li>
<li><p><strong>GENSIM</strong> - For seamless transformation from a high dimensional "bag of words" (BOW) feature space to a lower dimensional "topic" space (each topic expresses discovered relations amongst the words in the Tweet data set using Latent Semantic Indexing or LSI).</p></li>
<li><p><strong>Scikit-Learn</strong> - For trivial implementation the final Logistic Regression model which predicts "disaster" or "not disaster" based on each Tweets "topic" representation.</p></li>
</ol>

<h3>
<a id="2-clean-and-tokenize-all-the-tweets" class="anchor" href="#2-clean-and-tokenize-all-the-tweets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Clean and tokenize all the tweets</h3>

<hr>

<ol>
<li>Convert hyphens and apostrophes from utf8 to ascii</li>
<li>Remove all remaining utf8 characters</li>
<li>Clean any HTML tags using HTMLParser module</li>
<li>Break on hyphens</li>
<li>Employ NLTK for initial tweet tokenization

<ul>
<li>Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)</li>
</ul>
</li>
<li>Introduce special tokens

<ul>
<li>
<strong>|-num-|</strong> for numbers (detect comma separation as in 1,000,000)</li>
<li>
<strong>|-num_alpha-|</strong> for mixed numerical and alphabetical (interstates, planes, trains) but not handles </li>
<li>
<strong>|-num_units-|</strong> for zero, one, ..., ten</li>
<li>
<strong>|-num_tens-|</strong> for ten, twenty, ..., ninety</li>
<li>
<strong>|-num_scales-|</strong> for hundred, thousand, ..., billion</li>
<li>
<strong>|-website-|</strong> for any hyperlinks</li>
</ul>
</li>
<li>Simplify common face emoticons down to just eyes and mouth (nose does not really convey emotion) and normalize (eyes first mouth second)

<ul>
<li>Most common eyes : ; = 8</li>
<li>Most common mouths ( ) [ ] d p { } / @ |</li>
</ul>
</li>
<li>Stem words using NLTK Porter stemmer (e.g., fires becomes fire)</li>
</ol>

<p><strong>NOTE:</strong> stop words (like "and") are not removed as keeping them improves performance--perhaps by allowing for better word-word relations to be encoded in the discovered "topics".</p>

<h3>
<a id="3-mapping-low-frequency-words-onto-high-frequency-analogs" class="anchor" href="#3-mapping-low-frequency-words-onto-high-frequency-analogs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Mapping low frequency words onto high frequency analogs</h3>

<hr>

<p><strong>Low frequency words</strong>: occur <em>once</em> in the whole cleaned and tokenized tweet data set.</p>

<p><strong>High frequency words</strong>: occur greater than <em>once</em> in the whole cleaned and tokenized tweet data set.</p>

<p><strong>NOTE</strong>: low frequency words also encompass those with no occurrence (as in a new tweet outside of the dataset). Word2vec would be used to map in that case as well.</p>

<p>I use the pre-trained word2vec vectors from the Google News data set to quantify the similarity between two words. Low frequency <em>stemmed</em> tokens are replaced by most similar high frequency <em>stemmed</em> token (see cartoon).</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure.png" alt="mapping_visual2"></p>

<h3>
<a id="4-converting-tweets-into-features-for-modeling" class="anchor" href="#4-converting-tweets-into-features-for-modeling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Converting tweets into features for modeling</h3>

<hr>

<p><strong>Some useful definitions:</strong>
1. <strong>Bag of words (BOW)</strong>:  this is a vector representation of the form [1,0,0,2,4,0,...,0] where each dimension denotes a particular word and the number specifies how many times that word appears in a particular document (tweet in this case). There is a dimension for every word that appears at least twice (in this case) in the whole "corpus" of tweets (this is our "dictionary"). Since tweets are short the vectors are very sparse (mostly full of 0's). Please see <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">https://en.wikipedia.org/wiki/Bag-of-words_model</a> for more details.
2. <strong>Term frequencyâ€“inverse document frequency (TFIDF)</strong>: rescaled BOW vectors where each word count is penalized (shrunk) in accord with how many of the documents contains the word--effectively adjusting the word counts for how unique each word is. For example, "and" will be heavily penalized, reflecting low information content and common usage. Since this is just a rescaling of each BOW element the new vectors are of the same <em>high</em> dimensionality. Please see <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> for more details.
3. <strong>Latent semantic indexing (LSI)</strong>: through a standard mathematical scheme from linear algebra--singular value decomposition (SVD--the TFIDF vectors are mapped onto a user specified, smaller dimensional space where the new dimensions are linear combinations of the old TFIDF dimensions. These new dimensions (commonly called "topics") are constructed so that they describe as much variance in the data as possible with rapidly diminishing returns for each new dimension. The outcome is a much more compact vectorial representation of each tweet. Please see for <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a> more details.</p>

<p><strong>Workflow</strong>
1. Create a dictionary and corpus</p>

<ul>
<li>Dictionary: use words from all of the tweets for more tokens</li>
<li>Corpus: composed of the tweets with the greatest certainty in their classification. Convert to a BOW representation using the dictionary and then to a TFIDF analog.

<ol>
<li>Create LSI features data frame of desired dimensionality from the dictionary and TFIDF tweet corpus (the dictionary just relates the new features to the actual words and is not really part of the dimensionality reduction)</li>
</ol>
</li>
</ul>

<p><strong>Create data frames for constructing the dictionary and corpus</strong></p>

<p><strong>Dictionary</strong>: use all the tweets to get maximal word recognition, thus maximizing generalizability to new tweets. Tokens that appear only once are excluded.</p>

<p><strong>Corpus</strong>: only employ tweets of 100% confidence in classification to ensure the reliability of the predictive model. Some of the tweets with less than 100% appeared incorrectly labeled upon personal inspection.</p>

<h3>
<a id="5-decide-on-a-model-dimensionality-and-check-accuracy" class="anchor" href="#5-decide-on-a-model-dimensionality-and-check-accuracy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Decide on a model dimensionality and check accuracy</h3>

<hr>

<p><strong>Model used</strong>: logistic regression (see: <a href="https://en.wikipedia.org/wiki/Logistic_regression">https://en.wikipedia.org/wiki/Logistic_regression</a>).</p>

<p><strong>Why</strong>: well suited to high dimensional problems.</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation.png" alt="cross validation"></p>

<p><strong>Conclusion</strong>: Overall, D=250 seems good and is the dimensionality employed from here onward.</p>

<h3>
<a id="7-make-and-test-a-model-with-desired-dimensionality" class="anchor" href="#7-make-and-test-a-model-with-desired-dimensionality" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>7. Make and test a model with desired dimensionality</h3>

<hr>

<ol>
<li>Create a k-fold receiver operating characteristic (ROC) curve (see: <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a> for more details).

<ul>
<li>This checks how well the model "separates" the two classes by plotting the "true positive rate" vs the "false positive rate" while moving the probability cutoff for classifying into one of the two categories (the default for classification is 50%).</li>
</ul>
</li>
</ol>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/roc.png" alt="ROC analysis"></p>

<p>The ROC analysis suggests the model does a good job at separating out the classes. </p>

<h3>
<a id="8-check-out-the-topics" class="anchor" href="#8-check-out-the-topics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Check out the "topics"</h3>

<hr>

<ol>
<li>Print out the top 10 topics with the top ten tokens they are composed of</li>
<li>Plot some topics against each other with colors to indicate class</li>
</ol>

<p><code>Topic 0: 0.426*"?" + 0.234*"#" + 0.205*"@" + 0.196*"|-no_w2v-|" + 0.188*"'" + 0.185*"." + 0.183*"the" + 0.160*"|-num-|" + 0.158*"i" + 0.156*"a"</code></p>

<p><code>Topic 1: -0.808*"?" + 0.160*"|-num-|" + 0.159*"'" + 0.148*":" + 0.127*"#" + 0.110*"of" + 0.107*"in" + 0.096*"|-website-|" + 0.093*"famili" + 0.093*"..."</code></p>

<h3>
<a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>...</h3>

<p><code>Topic 9: -0.242*"kill" + 0.231*"#" + 0.222*"!" + 0.212*"|-num_alpha-|" + -0.208*"obama" + -0.203*"declar" + -0.203*"disast" + -0.173*"|-num_units-|" + 0.167*"bomb" + -0.150*"for"</code></p>

<p><code>Topic 10: -0.463*"!" + -0.321*"obama" + -0.312*"declar" + -0.287*"disast" + -0.208*"for" + -0.200*"@" + -0.185*"saipan" + -0.184*"typhoon" + 0.180*"." + -0.166*"devast"</code></p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics.png" alt="Topics"></p>

<h3>
<a id="8-proposed-work" class="anchor" href="#8-proposed-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Proposed work</h3>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/rjadrich">rjadrich</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
