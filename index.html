<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Disaster detection via twitter by rjadrich</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Disaster detection via twitter</h1>
        <p>Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)</p>
        <p class="view"><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View the Project on GitHub <small>rjadrich/disaster_detection_via_twitter</small></a></p>

        <ul>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View On <strong>GitHub</strong></a></li>
        </ul>
        
        <hr>
        
        <p><strong>Section links</strong></p>
        <p class="view"><a href="#3-mapping-low-frequency-words-onto-high-frequency-analogs">Map low frequency tokens</a></p>
        
        
        
      </header>
      <section>
        <h3>
<a id="1-project-summary" class="anchor" href="#1-project-summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Project summary</h3>

<hr>

<p><strong>Goal:</strong> Create a predictive model to classify a tweet as pertaining to a disaster event or not, <em>solely</em> based on the tweet text.</p>

<p><strong>Data:</strong> 10,876 classified tweet entries in the "Disasters on Social Media" data set from CrowdFlower at <a href="http://www.crowdflower.com/data-for-everyone">http://www.crowdflower.com/data-for-everyone</a>. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.</p>

<p><strong>Machine learning tools</strong> </p>

<ol>
<li><p><strong>Natural Language Toolkit (NLTK)</strong> - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.</p></li>
<li><p><strong>GENSIM</strong> - For seamless transformation from a high dimensional "bag of words" (BOW) feature space to a lower dimensional "topic" space (each topic expresses discovered relations amongst the words in the Tweet data set using Latent Semantic Indexing or LSI).</p></li>
<li><p><strong>Scikit-Learn</strong> - For trivial implementation the final Logistic Regression model which predicts "disaster" or "not disaster" based on each Tweets "topic" representation.</p></li>
</ol>

<h3>
<a id="2-clean-and-tokenize-all-the-tweets" class="anchor" href="#2-clean-and-tokenize-all-the-tweets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Clean and tokenize all the tweets</h3>

<hr>

<ol>
<li>Convert hyphens and apostrophes from utf8 to ascii</li>
<li>Remove all remaining utf8 characters</li>
<li>Clean any HTML tags using HTMLParser module</li>
<li>Break on hyphens</li>
<li>Employ NLTK for initial tweet tokenization

<ul>
<li>Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)

<ul>
<li>In general, the english language has at most three identical letters in a row</li>
</ul>
</li>
</ul>
</li>
<li>Introduce special tokens

<ul>
<li>
<strong>|-num-|</strong> for numbers (detect comma separation as in 1,000,000)</li>
<li>
<strong>|-num_alpha-|</strong> for mixed numerical and alphabetical (maybe useful for interstates, planes, trains, ...) 

<ul>
<li>Make sure not to convert handles! These are typically mixed numeric and alphabetic</li>
</ul>
</li>
<li>
<strong>|-num_units-|</strong> for zero, one, ..., ten</li>
<li>
<strong>|-num_tens-|</strong> for ten, twenty, ..., ninety</li>
<li>
<strong>|-num_scales-|</strong> for hundred, thousand, ..., billion</li>
<li>
<strong>|-website-|</strong> for any hyperlinks</li>
</ul>
</li>
<li>Simplify common face emoticons down to just eyes and mouth (nose does not really convey emotion) and normalize (eyes first mouth second)

<ul>
<li>Most common eyes : ; = 8</li>
<li>Most common mouths ( ) [ ] d p { } / @ |</li>
</ul>
</li>
<li>Stem words using NLTK Porter stemmer (e.g., fires becomes fire)</li>
</ol>

<p><strong>NOTE:</strong> stop words (i.e., high frequency words like "and") are not removed. Keeping them helps improve the final model performance--perhaps by allowing for better word-word relations to be encoded in the discovered "topics".</p>

<h3>
<a id="3-mapping-low-frequency-words-onto-high-frequency-analogs" class="anchor" href="#3-mapping-low-frequency-words-onto-high-frequency-analogs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Mapping low frequency words onto high frequency analogs</h3>

<hr>

<p><strong>Low frequency words</strong>: occur <em>once</em> in the whole cleaned and tokenized tweet data set.</p>

<p><strong>High frequency words</strong>: occur greater than <em>once</em> in the whole cleaned and tokenized tweet data set.</p>

<p><strong>NOTE</strong>: low frequency words also encompass those with no occurrence (as in a new tweet outside of the dataset). Word2vec would be used to map in that case as well.</p>

<p>For this I employ the pre-trained word2vec vectors from the Google News dataset. Word2vec is an unsupervised machine learning tool that creates vector representations of words by analyzing a large text corpus (here, Google News). Importantly, the similarity between two words can be quantified by the angle (theta in cartoon below) between the two word vectors.</p>

<p>Mapping a low frequency <em>stemmed</em> token via word2vec amounts to finding the most similar high frequency <em>stemmed</em> token by comparing all base <em>unstemmed</em> words. This procedure is depicted in the following cartoon where arrows represent a mapping and some actual examples (from mapping procedure carried out below) are shown.</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure.png" alt="mapping_visual2"></p>

<h3>
<a id="6-decide-on-a-model-dimensionality-and-check-accuracy" class="anchor" href="#6-decide-on-a-model-dimensionality-and-check-accuracy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>6. Decide on a model dimensionality and check accuracy</h3>

<hr>

<p><strong>Model used</strong>: logistic regression (see: <a href="https://en.wikipedia.org/wiki/Logistic_regression">https://en.wikipedia.org/wiki/Logistic_regression</a>).</p>

<p><strong>Why</strong>: well suited to high dimensional problems.</p>

<p><strong>Two tests across a range of dimensions</strong>:</p>

<ol>
<li>k-fold cross validation error to assess how well the model will generalize to new tweets.

<ul>
<li>Unfortunately this does not account well for testing generalizability to tweets with new (non-dictionary) tokens. Ideally, the word2vec mapping will remedy the relatively small dictionary.</li>
</ul>
</li>
<li>Training error to fit the whole dataset.</li>
</ol>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation.png" alt="cross validation"></p>

<h3>
<a id="8-check-out-the-topics" class="anchor" href="#8-check-out-the-topics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Check out the "topics"</h3>

<hr>

<ol>
<li>Print out the top 10 topics with the top ten tokens they are composed of</li>
<li>Plot some topics against each other with colors to indicate class</li>
</ol>

<p><code>Topic 0: 0.426*"?" + 0.234*"#" + 0.205*"@" + 0.196*"|-no_w2v-|" + 0.188*"'" + 0.185*"." + 0.183*"the" + 0.160*"|-num-|" + 0.158*"i" + 0.156*"a"</code></p>

<p><code>Topic 1: -0.808*"?" + 0.160*"|-num-|" + 0.159*"'" + 0.148*":" + 0.127*"#" + 0.110*"of" + 0.107*"in" + 0.096*"|-website-|" + 0.093*"famili" + 0.093*"..."</code></p>

<h3>
<a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>...</h3>

<p><code>Topic 9: -0.242*"kill" + 0.231*"#" + 0.222*"!" + 0.212*"|-num_alpha-|" + -0.208*"obama" + -0.203*"declar" + -0.203*"disast" + -0.173*"|-num_units-|" + 0.167*"bomb" + -0.150*"for"</code></p>

<p><code>Topic 10: -0.463*"!" + -0.321*"obama" + -0.312*"declar" + -0.287*"disast" + -0.208*"for" + -0.200*"@" + -0.185*"saipan" + -0.184*"typhoon" + 0.180*"." + -0.166*"devast"</code></p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics.png" alt="Topics"></p>

<h3>
<a id="8-proposed-work" class="anchor" href="#8-proposed-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Proposed work</h3>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/rjadrich">rjadrich</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
