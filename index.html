<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Disaster Detection via Twitter by rjadrich</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Disaster Detection via Twitter</h1>
        <p>Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)</p>

        <p class="view"><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View the Project on GitHub <small>rjadrich/disaster_detection_via_twitter</small></a></p>


        <ul>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View On <strong>GitHub</strong></a></li>
        </ul>
        
        
         <hr>
        
        
        
        <h3><a id="project-links" class="anchor" href="#project-links" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sections</h3>
        <p class="view">
          <a href="#1-project-summary">Project summary | </a>
          <a href="#2-tweet-preparation">Tweet preparation | </a>
          <a href="#3-mapping-uncommon-words">Mapping uncommon words | </a>
          <a href="#4-tweet-topic-generation">Tweet topic generation |</a>
          <a href="#5-accuracy-and-number-of-topics">Accuracy and number of topics | </a>
          <a href="#6-model-class-separability">Model class separability | </a>
          <a href="#7-explore-the-topics">Explore the topics | </a>
          <a href="#8-improving-tokenization">Improving tokenization | </a>
          <a href="#9-improving-topic-vocabulary">Improving topic vocabulary</a>
        </p>
        
        
        
        
        
      </header>
      <section>
        <hr>

<h3>
<a id="1-project-summary" class="anchor" href="#1-project-summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Project summary</h3>

<hr>

<p><strong>Goal:</strong> Create a predictive model to classify a tweet as pertaining to a disaster event or not, <em>solely</em> based on the tweet text.</p>

<p><strong>Data:</strong> 10,876 classified tweet entries in the "Disasters on Social Media" data set from CrowdFlower at <a href="http://www.crowdflower.com/data-for-everyone">http://www.crowdflower.com/data-for-everyone</a>. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.</p>

<p><strong>Machine learning tools</strong> </p>

<ol>
<li><p><strong>Natural Language Toolkit (NLTK)</strong> - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.</p></li>
<li><p><strong>GENSIM</strong> - For transforming the high dimensional "bag of words" (BOW) feature space to a lower dimensional "topic" space via Latent Semantic Indexing or LSI.</p></li>
<li><p><strong>Scikit-Learn</strong> - Fitting of final Logistic Regression classification model to tweets in "topic" form.</p></li>
</ol>

<p> </p>

<hr>

<h3>
<a id="2-tweet-preparation" class="anchor" href="#2-tweet-preparation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Tweet preparation</h3>

<hr>

<ol>
<li>Convert hyphens and apostrophes from utf8 to ascii</li>
<li>Remove all remaining utf8 characters</li>
<li>Clean any HTML tags using HTMLParser module</li>
<li>Break on hyphens</li>
<li>Employ NLTK for initial tweet tokenization

<ul>
<li>Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)</li>
<li>@ and # are separated from text</li>
</ul>
</li>
<li>
<strong>(New)</strong> Split @ and # text using <a href="#8-improving-tokenization">token splitting tool</a>
</li>
<li>
<strong>(New)</strong> Map <a href="http://www.datagenetics.com/blog/october52012/index.html">top 90%</a> of emoticons to <strong>|-happy-|</strong>, <strong>|-sad-|</strong> or <strong>|-inert-|</strong> tokens</li>
<li>Introduce special tokens

<ul>
<li>
<strong>|-num-|</strong> for numbers (detect comma separation as in 1,000,000)</li>
<li>
<strong>|-num_alpha-|</strong> for mixed numerical and alphabetical (interstates, planes, trains) but not handles </li>
<li>
<strong>|-num_units-|</strong> for zero, one, ..., ten</li>
<li>
<strong>|-num_tens-|</strong> for ten, twenty, ..., ninety</li>
<li>
<strong>|-num_scales-|</strong> for hundred, thousand, ..., billion</li>
<li>
<strong>|-website-|</strong> for any hyperlinks</li>
</ul>
</li>
<li>Stem words using NLTK Porter stemmer (e.g., fires becomes fire)</li>
</ol>

<p><strong>NOTE:</strong> stop words (like "and") are not removed as keeping them improves performance--perhaps by allowing for better word-word relations to be encoded in the discovered "topics".</p>

<p> </p>

<hr>

<h3>
<a id="3-mapping-uncommon-words" class="anchor" href="#3-mapping-uncommon-words" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Mapping uncommon words</h3>

<hr>

<p><strong>Low frequency words</strong>: occur <em>once</em> in the whole cleaned and tokenized tweet data set (also for unknown words).</p>

<p><strong>High frequency words</strong>: occur greater than <em>once</em> in the whole cleaned and tokenized tweet data set.</p>

<p>Leverage word2vec and pre-trained Google News vectors to quantify the similarity between two words. Low frequency <em>stemmed</em> tokens are replaced by most similar high frequency <em>stemmed</em> token (see cartoon).</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure-adv_pares.png" alt="mapping_visual2"></p>

<p>Employ special <strong>|-no_w2v-|</strong> token for word2vec unrecognized words.</p>

<p> </p>

<hr>

<h3>
<a id="4-tweet-topic-generation" class="anchor" href="#4-tweet-topic-generation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Tweet topic generation</h3>

<hr>

<p><strong>Steps:</strong></p>

<ol>
<li>
<strong>Bag of words (BOW)</strong>:  vectors like [1,0,0,2,4,0,...,0] where each count specifies how many time a certain token appears in a tweet (sparse, i.e., mostly full of 0's).</li>
<li>
<strong>Term frequency–inverse document frequency (TFIDF)</strong>: rescaled BOW vectors where each word count is penalized (shrunk) in accord with how many of the documents contains the word (measure of uniqueness).</li>
<li>
<strong>Latent semantic indexing (LSI)</strong>: constructs <em>user specified</em> number of "topics" (linear combinations of the old TFIDF dimensions). These are constructed so so as to describe as much variance in the data as possible. </li>
</ol>

<p><strong>Dictionary</strong>: tokens from <em>all</em> the tweets for get maximal word recognition. Tokens that appear only once are excluded.</p>

<p><strong>Corpus</strong>: only tweets of 100% confidence in classification to maximize reliability (some tweets with &lt;100% appeared incorrectly labeled upon personal inspection).</p>

<p> </p>

<hr>

<h3>
<a id="5-accuracy-and-number-of-topics" class="anchor" href="#5-accuracy-and-number-of-topics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Accuracy and number of topics</h3>

<hr>

<p><strong>Model used</strong>: logistic regression (well suited to high dimensional problems).</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation_adv_parse.png" alt="cross validation"></p>

<p><strong>Conclusion</strong>: CV accuracy of ~88% near D=250. I use this dimensionality from here onward.</p>

<p> </p>

<hr>

<h3>
<a id="6-model-class-separability" class="anchor" href="#6-model-class-separability" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>6. Model class separability</h3>

<hr>

<p><strong>Receiver operating characteristic (ROC) curve:</strong></p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/roc_adv_parse.png" alt="ROC analysis"></p>

<p><strong>NOTE</strong>: May want to tune threshold towards more false disaster predictions (better safe than sorry).</p>

<p> </p>

<hr>

<h3>
<a id="7-explore-the-topics" class="anchor" href="#7-explore-the-topics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>7. Explore the topics</h3>

<hr>

<p><strong>Top 10 topics with the top ten tokens</strong></p>

<p><code>Topic 0: 0.403*"?" + 0.237*"#" + 0.196*"'" + 0.185*"the" + 0.184*"." + 0.177*"a" + 0.170*"i" + 0.168*"@" + 0.167*"|-num-|" + 0.149*":"</code></p>

<p><code>Topic 1: -0.826*"?" + 0.156*"'" + 0.147*"|-num-|" + 0.145*":" + 0.106*"#" + 0.105*"of" + 0.098*"famili" + 0.098*"in" + 0.092*"by" + 0.091*"|-website-|"</code></p>

<h3>
<a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>...</h3>

<p><code>Topic 5: 0.298*"suicid" + 0.266*"bomber" + 0.210*"bomb" + 0.201*"kill" + 0.191*"#" + 0.177*"mosqu" + 0.172*"saudi" + 0.172*"|-num-|" + 0.148*"?" + 0.146*"pkk"</code></p>

<p><code>Topic 6: 0.569*"#" + -0.198*"bomb" + -0.196*"suicid" + -0.184*"bomber" + -0.156*"pkk" + -0.152*"turkey" + -0.152*"trench" + -0.151*"deton" + -0.146*"old" + 0.140*"|-num-|"</code></p>

<p><strong>Topic 1 = non-disaster and  Topic 5 = disaster???</strong></p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics-adv_parse.png" alt="Topics"></p>

<p> </p>

<hr>

<h3>
<a id="8-improving-tokenization" class="anchor" href="#8-improving-tokenization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Improving tokenization</h3>

<hr>

<p><strong>Parsing conjoined words</strong> (common to hashtags)</p>

<p>In development since challenge (<a href="https://github.com/rjadrich/disaster_detection_via_twitter/blob/master/token_splicer_example.ipynb">check out the Brown corpus demo</a>) </p>

<ol>
<li>Corpus &gt; word probabilities, <code>P(w) ~ frequency</code>
</li>
<li>Recursively generate word splits with smoothing (unrecognized words = unit frequency)</li>
<li>Maximum likelihood monogram model: <code>log[P(w1, w2, ..., wN)] = log[P(w1)] + log[P(w2)] + ... + log[P(wN)]</code>
</li>
</ol>

<p>Final model will use the huge <a href="http://clic.cimec.unitn.it/amac/twitter_ngram/">Rovereto Twitter Corpus</a></p>

<p><strong>Spell checking</strong></p>

<ol>
<li>Use <a href="http://pythonhosted.org/pyenchant/tutorial.html">PyEnchant</a> to find most likely correction</li>
<li>Introduce <strong>|-m_spell-|</strong> for each misspelled word</li>
<li>Interface with conjoined word parser?</li>
</ol>

<p><strong>Named entity recognition</strong></p>

<ol>
<li>Employ Stanford Named Entity Recognition Tagger (via <a href="http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford">NLTK</a>)</li>
<li>Introduce tokens for <strong>persons</strong>, <strong>places</strong>, <strong>organizations</strong>, etc.</li>
</ol>

<p> </p>

<hr>

<h3>
<a id="9-improving-topic-vocabulary" class="anchor" href="#9-improving-topic-vocabulary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>9. Improving topic vocabulary</h3>

<hr>

<p><strong>Goal</strong>: amass significantly more unlabeled Tweets to directly enhance the number of recognizable words. </p>

<p><strong>How</strong>: Twitter API search using the same employed by CrowdFlower. </p>

<p><strong>Why does this work</strong>: If a topic encodes a correlation among the words “bombing, terrorist, suicide, explosive”, but only “bombing” is in the training set, the applicability of the other three words to a disaster event will be naturally captured by the model fitting to the Topic with a strong weight on “bombing”.</p>

<p>Twitter Firehose access would be great!</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/rjadrich">rjadrich</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
