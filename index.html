<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Disaster Detection via Twitter by rjadrich</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Disaster Detection via Twitter</h1>
        <p>Predictive classification model for determining if a Tweet is discussing a disaster event (i.e., building collapse, wildfire, terrorist attack)</p>

        <p class="view"><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View the Project on GitHub <small>rjadrich/disaster_detection_via_twitter</small></a></p>


        <ul>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/rjadrich/disaster_detection_via_twitter">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <hr>

<h3>
<a id="1-project-summary" class="anchor" href="#1-project-summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Project summary</h3>

<hr>

<p><strong>Goal:</strong> Create a predictive model to classify a tweet as pertaining to a disaster event or not, <em>solely</em> based on the tweet text.</p>

<p><strong>Data:</strong> 10,876 classified tweet entries in the "Disasters on Social Media" data set from CrowdFlower at <a href="http://www.crowdflower.com/data-for-everyone">http://www.crowdflower.com/data-for-everyone</a>. These were compiled by searching for tweets with disaster related keywords (i.e., hijacking, hurricane, and explosion) and then they were classified.</p>

<p><strong>Machine learning tools</strong> </p>

<ol>
<li><p><strong>Natural Language Toolkit (NLTK)</strong> - For initial preprocessing and tokenization of tweets using the nltk.tokenize package.</p></li>
<li><p><strong>GENSIM</strong> - For transforming the high dimensional "bag of words" (BOW) feature space to a lower dimensional "topic" space via Latent Semantic Indexing or LSI.</p></li>
<li><p><strong>Scikit-Learn</strong> - Fitting of final Logistic Regression classification model to tweets in "topic" form.</p></li>
</ol>

<p> </p>

<hr>

<h3>
<a id="2-tweet-preparation" class="anchor" href="#2-tweet-preparation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Tweet preparation</h3>

<hr>

<ol>
<li>Convert hyphens and apostrophes from utf8 to ascii</li>
<li>Remove all remaining utf8 characters</li>
<li>Clean any HTML tags using HTMLParser module</li>
<li>Break on hyphens</li>
<li>Employ NLTK for initial tweet tokenization

<ul>
<li>Keep handles (username) and hashtags and reduce length (e.g., looooooovvvvveeee to looovvveee)</li>
<li>@ and # are separated from text</li>
<li>Split # text according to normal format (#CarCrash or #TexasTornado)</li>
</ul>
</li>
<li>Introduce special tokens

<ul>
<li>
<strong>|-num-|</strong> for numbers (detect comma separation as in 1,000,000)</li>
<li>
<strong>|-num_alpha-|</strong> for mixed numerical and alphabetical (interstates, planes, trains) but not handles </li>
<li>
<strong>|-num_units-|</strong> for zero, one, ..., ten</li>
<li>
<strong>|-num_tens-|</strong> for ten, twenty, ..., ninety</li>
<li>
<strong>|-num_scales-|</strong> for hundred, thousand, ..., billion</li>
<li>
<strong>|-website-|</strong> for any hyperlinks</li>
</ul>
</li>
<li>Simplify common face emoticons down to just eyes and mouth (nose does not really convey emotion) and normalize (eyes first mouth second)

<ul>
<li>Most common eyes : ; = 8</li>
<li>Most common mouths ( ) [ ] d p { } / @ |</li>
</ul>
</li>
<li>Stem words using NLTK Porter stemmer (e.g., fires becomes fire)</li>
</ol>

<p><strong>NOTE:</strong> stop words (like "and") are not removed as keeping them improves performance--perhaps by allowing for better word-word relations to be encoded in the discovered "topics".</p>

<p> </p>

<hr>

<h3>
<a id="3-mapping-uncommon-words" class="anchor" href="#3-mapping-uncommon-words" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Mapping uncommon words</h3>

<hr>

<p><strong>Low frequency words</strong>: occur <em>once</em> in the whole cleaned and tokenized tweet data set (also for unknown words).</p>

<p><strong>High frequency words</strong>: occur greater than <em>once</em> in the whole cleaned and tokenized tweet data set.</p>

<p>Leverage word2vec and pre-trained Google News vectors to quantify the similarity between two words. Low frequency <em>stemmed</em> tokens are replaced by most similar high frequency <em>stemmed</em> token (see cartoon).</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/images/mapping_procedure.png" alt="mapping_visual2"></p>

<p>Employ special <strong>|-no_w2v-|</strong> token for word2vec unrecognized words.</p>

<p> </p>

<hr>

<h3>
<a id="4-tweet-topic-generation" class="anchor" href="#4-tweet-topic-generation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Tweet topic generation</h3>

<hr>

<p><strong>Some useful definitions:</strong></p>

<ol>
<li>
<strong>Bag of words (BOW)</strong>:  vectors like [1,0,0,2,4,0,...,0] where each count specifies how many time a certain token appears in a tweet (sparse, i.e., mostly full of 0's).</li>
<li>
<strong>Term frequency–inverse document frequency (TFIDF)</strong>: rescaled BOW vectors where each word count is penalized (shrunk) in accord with how many of the documents contains the word (measure of uniqueness).</li>
<li>
<strong>Latent semantic indexing (LSI)</strong>: constructs <em>user specified</em> number of "topics" (linear combinations of the old TFIDF dimensions). These are constructed so so as to describe as much variance in the data as possible. </li>
</ol>

<p><strong>Dictionary</strong>: tokens from <em>all</em> the tweets for get maximal word recognition. Tokens that appear only once are excluded.</p>

<p><strong>Corpus</strong>: only tweets of 100% confidence in classification to maximize reliability (some tweets with &lt;100% appeared incorrectly labeled upon personal inspection).</p>

<p> </p>

<hr>

<h3>
<a id="5-accuracy-and-number-of-topics" class="anchor" href="#5-accuracy-and-number-of-topics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Accuracy and number of topics</h3>

<hr>

<p><strong>Model used</strong>: logistic regression (well suited to high dimensional problems).</p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/cross_validation.png" alt="cross validation"></p>

<p><strong>Conclusion</strong>: Overall, D=250 seems good. I use this dimensionality from here onward.</p>

<p> </p>

<hr>

<h3>
<a id="6-model-class-separability" class="anchor" href="#6-model-class-separability" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>6. Model class separability</h3>

<hr>

<p><strong>Receiver operating characteristic (ROC) curve:</strong></p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/roc.png" alt="ROC analysis"></p>

<p><strong>NOTE</strong>: May want to tune threshold towards more false disaster predictions (better safe than sorry).</p>

<p> </p>

<hr>

<h3>
<a id="7-explore-the-topics" class="anchor" href="#7-explore-the-topics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>7. Explore the topics</h3>

<hr>

<p><strong>Top 10 topics with the top ten tokens</strong></p>

<p><code>Topic 0: 0.426*"?" + 0.234*"#" + 0.205*"@" + 0.196*"|-no_w2v-|" + 0.188*"'" + 0.185*"." + 0.183*"the" + 0.160*"|-num-|" + 0.158*"i" + 0.156*"a"</code></p>

<p><code>Topic 1: -0.808*"?" + 0.160*"|-num-|" + 0.159*"'" + 0.148*":" + 0.127*"#" + 0.110*"of" + 0.107*"in" + 0.096*"|-website-|" + 0.093*"famili" + 0.093*"..."</code></p>

<h3>
<a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>...</h3>

<p><code>Topic 9: -0.242*"kill" + 0.231*"#" + 0.222*"!" + 0.212*"|-num_alpha-|" + -0.208*"obama" + -0.203*"declar" + -0.203*"disast" + -0.173*"|-num_units-|" + 0.167*"bomb" + -0.150*"for"</code></p>

<p><code>Topic 10: -0.463*"!" + -0.321*"obama" + -0.312*"declar" + -0.287*"disast" + -0.208*"for" + -0.200*"@" + -0.185*"saipan" + -0.184*"typhoon" + 0.180*"." + -0.166*"devast"</code></p>

<p><strong>Topic 1 = non-disaster and  Topic 2 = disaster???</strong></p>

<p><img src="https://raw.githubusercontent.com/rjadrich/disaster_detection_via_twitter/master/data/cv_and_roc_data/topics.png" alt="Topics"></p>

<p> </p>

<hr>

<h3>
<a id="8-improving-tokenization" class="anchor" href="#8-improving-tokenization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Improving tokenization</h3>

<hr>

<p><strong>Parsing conjoined words</strong> (common to hashtags and "lazy" tweeters)</p>

<ol>
<li>Google book n-grams &gt; dictionary &gt; word probabilities, <code>P(w) ~ # w</code>
</li>
<li>Recursively generate word splits with smoothing (unrecognized words = unit frequency)</li>
<li>Maximum likelihood monogram model: <code>log[P(w1, w2, ..., wN)] = log[P(w1)] + log[P(w2)] + ... + log[P(wN)]</code>
</li>
<li>Special token for failure: <strong>|-no_parse-|</strong>
</li>
</ol>

<p><strong>Spell checking</strong> (again, "lazy" tweeters)</p>

<ol>
<li>Use <a href="http://pythonhosted.org/pyenchant/tutorial.html">PyEnchant</a> to find most likely correction</li>
<li>Introduce <strong>|-m_spell-|</strong> for each misspelled word</li>
</ol>

<p><em>Could interface with conjoined word parser</em></p>

<p><strong>Named entity recognition</strong></p>

<ol>
<li><p>Employ Stanford Named Entity Recognition Tagger (via <a href="http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford">NLTK</a>)</p></li>
<li><p>Introduce tokens for <strong>persons</strong>, <strong>places</strong>, <strong>organizations</strong>, etc.</p></li>
</ol>

<p><em>Will capture more general tweet structure</em></p>

<hr>

<h3>
<a id="8-broadening-the-topic-knowledge-base" class="anchor" href="#8-broadening-the-topic-knowledge-base" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Broadening the topic knowledge base</h3>

<hr>

<p>~10,000 Tweets (short) = good but not great vocabulary
 <em>Use word2vec mapping but only contains *</em>words**</p>

<p>Perhaps the greatest limitation of the current model is the limited vocabulary—a consequence of the relatively
small sample size and temporal specificity. I have already proposed—and demonstrated the implementation of—
one way to partially remedy this by utilizing Google’s word2vec program to map unknown words onto known words
by a similarity query (see full python notebook if interested). This does not capture nonword
entities though (such
as emoticons), thus I propose to build upon this strategy by amassing significantly more unlabeled Tweets to
directly enhance the number of recognizable words. This will be accomplished via a Twitter API search for
disasterrelated
keywords using the same keywords employed by CrowdFlower in developing their dataset. With
this significantly augmented data set, I will form new, more robust Topics with a greatly expanded vocabulary for
use in the logistic regression. Topics encode wordword
relations in the tweets; therefore, if a topic encodes a
correlation among the words “bombing, terrorist, suicide, explosive”, but only “bombing” is in the training set, the
applicability of the other three words to a disaster event will be naturally captured by the model fitting to the Topic
with a strong weight on “bombing”.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/rjadrich">rjadrich</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
